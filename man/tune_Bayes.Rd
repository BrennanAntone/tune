% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune_Bayes.R
\name{tune_Bayes}
\alias{tune_Bayes}
\alias{tune_Bayes.default}
\alias{tune_Bayes.recipe}
\alias{tune_Bayes.formula}
\alias{tune_Bayes.workflow}
\title{Bayesian optimization of model parameters.}
\usage{
tune_Bayes(object, ...)

\method{tune_Bayes}{default}(object, ...)

\method{tune_Bayes}{recipe}(object, model, resamples, iter = 10,
  param_info = NULL, metrics = NULL, objective = exp_improve(),
  initial = NULL, control = ctrl_Bayes(), ...)

\method{tune_Bayes}{formula}(formula, model, resamples, iter = 10,
  param_info = NULL, metrics = NULL, objective = exp_improve(),
  initial = NULL, control = ctrl_Bayes(), ...)

\method{tune_Bayes}{workflow}(object, model = NULL, resamples,
  iter = 10, param_info = NULL, metrics = NULL,
  objective = exp_improve(), initial = NULL, control = ctrl_Bayes(),
  ...)
}
\arguments{
\item{object}{A model workflow, R formula or recipe object.}

\item{...}{Not currently used.}

\item{model}{A \code{parsnip} model specification (or \code{NULL} when \code{object} is a
workflow).}

\item{resamples}{An \code{rset()} object. This argument \strong{should be named}.}

\item{iter}{The maximum number of search iterations.}

\item{param_info}{A \code{dials::parameters()} object or \code{NULL}. If none is given,
a parameters set is derived from other arguments.}

\item{metrics}{A \code{yardstick::metric_set()} object containing information on how
models will be evaluated for performance. The first metric in \code{metrics} is the
one that will be optimized.}

\item{objective}{A character string for what metric should be optimized or
an acquisition function object.}

\item{initial}{An initial set of results in a tidy format.}

\item{control}{A control object created by \code{ctrl_Bayes()}}

\item{formula}{A traditional model formula.}
}
\value{
A tibble of results that mirror those generated by \code{tune_grid()}.
However, these results contain an \code{.iter} column and replicate the \code{rset}
object multiple times over iterations (at limited additional memory costs).
}
\description{
\code{tune_Bayes()} uses models to generate new candidate tuning parameter
combinations based on previous results.
}
\details{
The optimization starts with a set of initial results, such as those
generated by \code{tune_grid()}. If none exist, the function will create several
combinations and obtain their performance estimates.

Using one of the performance estimates as the \emph{model outcome}, a Gaussian
process (GP) model is created where the previous tuning parameter combinations
are used as the predictors.

A large grid of potential hyperparameter combinations is predicted using
the model and scored using an \emph{acquisition function}. These functions
usually combine the predicted mean and variance of the GP to decide the best
parameter combination to try next. For more information, see the
documentation for \code{exp_improve()} and the corresponding package vignette.

The best combination is evaluated using resampling and the process continues.
}
\section{Parallel Processing}{


The \code{foreach} package is used here. To execute the resampling iterations in
parallel, register a parallel backend function. See the documentation for
\code{foreach::foreach()} for examples.

For the most part, warnings generated during training are shown as they occur
and are associated with a specific resample when \code{control(verbose = TRUE)}.
They are (usually) not aggregated until the end of processing.

For Bayesian optimization, parallel processing is used to estimate the
resampled performance values once a new candidate set of values are estimated.
}

\section{Initial Values}{


The results of \code{tune_grid()}, or a previous run of \code{tune_Bayes()} can be used
in the \code{initial} argument.
}

\section{Performance Metrics}{


To use your own performance metrics, the \code{yardstick::metric_set()} function
can be used to pick what should be measured for each model. If multiple
metrics are desired, they can be bundled. For example, to estimate the area
under the ROC curve as well as the sensitivity and specificity (under the
typical probability cutoff of 0.50), the \code{metrics} argument could be given:

\preformatted{
  metrics = metric_set(roc_auc, sens, spec)
}

Each metric is calculated for each candidate model.

If no metric set is provided, one is created:
\itemize{
\item For regression models, the root mean squared error and coefficient
of determination are computed.
\item For classification, the log-likelihood and overall accuracy are
computed.
}

Note that the metrics also determine what type of predictions are estimated
during tuning. For example, in a classification problem, if metrics are used
that are all associated with hard class predictions, the classification
probabilities are not created.

The out-of-sample estimates of these metrics are contained in a list column
called \code{.metrics}. This tibble contains a row for each metric and columns
for the value, the estimator type, and so on.

An \code{estimate()} method can be used for these objects to collapse the results
over the resampled (to obtain the final resampling estimates per tuning
parameter combination).
}

\section{Obtaining Predictions}{


When \code{control(save_preds = TRUE)}, the output tibble contains a list column
called \code{.predictions} that has the out-of-sample predictions for each
parameter combination in the grid and each fold (which can be very large).

The elements of the tibble are tibbles with columns for the tuning
parameters, the row number from the original data object (\code{.row}), the
outcome data (with the same name(s) of the original data), and any columns
created by the predictions. For example, for simple regression problems, this
function generates a column called \code{.pred} and so on. As noted above, the
prediction columns that are returned are determined by the type of metric(s)
requested.

This list column can be \code{unnested} using \code{tidyr::unnest()} or using the
convenience function \code{collect_predictions()}.
}

\section{Extracting information}{


The \code{extract} control option will result in an additional function to be
returned called \code{.extracts}. This is a list column that has tibbles
containing the results of the user's function for each tuning parameter
combination. This can enable returning each model and/or recipe object that
is created during resampling. Note that this could result in a large return
object, depending on what is returned.
}

\seealso{
\code{ctrl_Bayes()}, \code{tune()}, \code{estimate.tune_results()},
\code{autoplot.tune_results()}, \code{show_best()}, \code{select_best()},
\code{collect_predictions()}, \code{collect_metrics()}, \code{prob_improve()},
\code{exp_improve()}, \code{conf_bound()}
}
